{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0cd5470",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Processamento de Dados em Larga Escala com Spark\n",
    "\n",
    "## Informação Básica\n",
    "- **Título do Projeto**: Processamento de Dados em Larga Escala com Spark\n",
    "- **Alunos**:\n",
    "  - João Carneiro, Nº 50938\n",
    "  - Eduardo Abrantes, Nº 50391\n",
    "\n",
    "## Contribuição\n",
    "\n",
    "| Aluno        | Tarefa realizada                                                                 | Horas estimadas |\n",
    "|--------------|----------------------------------------------------------------------------------|-----------------|\n",
    "| João Carneiro   | ...  | 7h             |\n",
    "| Eduardo Abrantes  | ...        | 7h             |\n",
    "\n",
    "## Background e Motivação\n",
    "\n",
    "- Este projeto tem como objetivo ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5042a-c407-4407-9f5c-ac99237c386f",
   "metadata": {},
   "source": [
    "# Pré-Requisitos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4938593-0d6d-4fb9-b501-5547f899b624",
   "metadata": {},
   "source": [
    "## Instalação dos Dados\n",
    "Antes de iniciar o projeto, é necessário realizar o download e preparação dos Dados Meteorológicos do GHCN-Daily. Siga os passos abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48770d8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup de Dependências\n",
    "Para a realização deste projeto, foram utilizadas bibliotecas fundamentais do Python para Ciência de Dados. A instalação das dependências foi realizada através do comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20429e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikiextractor\n",
      "  Downloading wikiextractor-3.0.6-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wikiextractor\n",
      "Successfully installed wikiextractor-3.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install wikiextractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d622916-9beb-4454-aae4-209bfe64ffb1",
   "metadata": {},
   "source": [
    "#### Descrição das Bibliotecas\n",
    "\n",
    "- wikiextractor: ...\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab753d14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extração do Dump da Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e67a4c-d214-41d8-a8b1-3c05e1960f43",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "O seguinte código foi utilizado para esta operação inicial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa29a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Starting page extraction from /home/jovyan/work/proj-three-data-science/data/enwiki-latest-pages-articles.xml.bz2.\n",
      "INFO: Using 11 extract processes.\n",
      "INFO: Extracted 100000 articles (633.2 art/s)\n"
     ]
    }
   ],
   "source": [
    "!python -m wikiextractor.WikiExtractor \"/home/jovyan/work/proj-three-data-science/data/enwiki-latest-pages-articles.xml.bz2\" -o \"/home/jovyan/work/proj-three-data-science/data/wikipedia-dump/text\" --no-templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634bc837-cc95-4d19-9f57-a273d49bd44c",
   "metadata": {},
   "source": [
    "## Leitura dos Arquivos com PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e38d8c0-2ea8-48ef-94c1-cd2b437779a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WikipediaDump\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Lê todos os arquivos de texto extraídos\n",
    "rdd = sc.wholeTextFiles(\"/home/jovyan/work/proj-three-data-science/data/wikipedia-dump/text/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba07db-f2e2-4f5a-bc04-8ceb98d70eed",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16160249-b314-4ec0-ba40-02096f677467",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercício 1 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41306e88-9214-4a56-a532-ef87cc8569b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_pages(file_content):\n",
    "    # file_content = (filename, text)\n",
    "    text = file_content[1]\n",
    "    docs = re.findall(r\"<doc(.*?)</doc>\", text, re.DOTALL)\n",
    "    result = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Extrai atributos (url e title) do cabeçalho do <doc>\n",
    "        header = re.search(r'url=\"(.*?)\".*?title=\"(.*?)\">', doc) \n",
    "        if not header:\n",
    "            continue\n",
    "        url = header.group(1)\n",
    "        title = header.group(2)\n",
    "\n",
    "        # Extrai conteúdo da página após o header\n",
    "        content = re.search(r'\">(.*?)$', doc, re.DOTALL)\n",
    "        if not content:\n",
    "            continue\n",
    "        content = content.group(1).strip()\n",
    "\n",
    "        # Apenas páginas com título e conteúdo\n",
    "        if title and content:\n",
    "            result.append((url, title, content))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b59064-2a4f-4a91-b008-dceb81f341e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.flatMap(extract_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11291050-734b-4cf6-9ae0-bce33ce755c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercício 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df649a-1591-4398-a963-589743f3155f",
   "metadata": {},
   "source": [
    "### Exercício 2.1 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95adc011-ad88-4e92-a23e-de3f1018cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d2d0a-9a37-4ebb-a685-ea0cf944fdc6",
   "metadata": {},
   "source": [
    "### Exercício 2.2 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c9043-073e-4eef-b7f6-e284610ef746",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a41e7-3ed9-4782-bad8-8107eb7bbd53",
   "metadata": {},
   "source": [
    "### Exercício 2.3 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741727b7-bda6-4214-8963-5f1ed93bb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "contagem_caracteres_rdd = rdd.map(lambda x: (x[0], x[1], len(x[2])))\n",
    "all_contagem_caracteres = contagem_caracteres_rdd.collect()\n",
    "\n",
    "for url, title, contagem_caracteres in all_contagem_caracteres:\n",
    "    print(f\"URL: {url}\\nTítulo: {title}\\nNº caracteres: {contagem_caracteres}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039597af-d28c-4efe-9fd5-b1a85c1774ce",
   "metadata": {},
   "source": [
    "### Exercício 2.4 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c280772-0570-47a5-a028-ed4833d29fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_caracteres = rdd.map(lambda x: len(x[2])).sum()\n",
    "print(\"Total de caracteres:\", total_caracteres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ad5c6-5e8f-4f06-8521-a763c8da0121",
   "metadata": {},
   "source": [
    "### Exercício 2.5 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107590a0-39f9-400c-9f78-30c8a5a75fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Compilar regex para melhor performance\n",
    "month_name = r\"(?:January|February|March|April|May|June|July|August|September|October|November|December)\"\n",
    "regex_1 = re.compile(rf\"{month_name} \\d{{1,2}}, \\d{{4}}\")      # Month Date, Year: Like August 12, 1993\n",
    "regex_2 = re.compile(rf\"\\d{{1,2}} {month_name} \\d{{4}}\")        # Date Month Year: Like 12 August 1993\n",
    "\n",
    "def is_celebrity(page):\n",
    "    content_start = page[2][:50]\n",
    "    return bool(regex_1.search(content_start) or regex_2.search(content_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0b071-5df3-4de3-bed3-2fc74f65fca5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercício 3 - Criação de um DataFrame com as Informações das Celebridades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276ffdf-08a6-4aea-a736-e5c1436d5b4c",
   "metadata": {},
   "source": [
    "Em primeiro, filtramos as páginas do RDD para obter as páginas que indicam celebridades, com base na data de nascimento nos primeiros 50 caracteres do conteúdo. Em segundo, os resultados foram convertidos para um DataFrame Spark com as colunas url, title e content, e por final transformamos num DataFrame Pandas para a visualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c553f-5259-4344-a481-0f3010870d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "celebridades_df = (\n",
    "    pages_rdd.filter(is_celebrity)\n",
    "             .toDF([\"url\", \"title\", \"content\"])\n",
    ")\n",
    "\n",
    "celebridades_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1582c6-5b43-4a53-91fe-039f5ddb69ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercício 4 - Guardar o resultado completo em um ficheiro `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408e54c-5114-4d67-9d06-68a3226c684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "celebridades_df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"celebridades.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c028b45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Declaração de Integridade\n",
    "\n",
    "> Eu, João Carneiro, estudante com o número de inscrição 50938 do 1º Ciclo em Informática Web, Móvel e na Nuvem da Universidade da Beira Interior, declaro ter desenvolvido o presente trabalho e elaborado o presente texto em total consonância com o Código de Integridade da Universidade da Beira Interior. Mais concretamente afirmo não ter incorrido em qualquer das variedades de Fraude Académica, e que aqui declaro conhecer, que em particular atendi à exigida referenciação de frases, extratos, imagens e outras formas de trabalho intelectual, e assumindo assim na íntegra as responsabilidades da autoria\n",
    "\n",
    "\n",
    "> Eu, Eduardo Abrantes, estudante com o número de inscrição 50391 do 1º Ciclo em Informática Web, Móvel e na Nuvem da Universidade da Beira Interior, declaro ter desenvolvido o presente trabalho e elaborado o presente texto em total consonância com o Código de Integridade da Universidade da Beira Interior. Mais concretamente afirmo não ter incorrido em qualquer das variedades de Fraude Académica, e que aqui declaro conhecer, que em particular atendi à exigida referenciação de frases, extratos, imagens e outras formas de trabalho intelectual, e assumindo assim na íntegra as responsabilidades da autoria\n",
    "\n",
    "Universidade da Beira Interior, Covilhã  \n",
    "02/06/2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e119a-83e5-4c10-a7e4-1f5b7fb2280c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
